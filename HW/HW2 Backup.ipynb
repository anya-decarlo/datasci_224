{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime as dt, numpy as np, pandas as pd\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# assumes fit_gradient_tree(X, gradients, max_depth, random_state) is already defined\n",
    "\n",
    "def fit_gradient_boosting(X, y, n_iters, max_depth, learning_rate,\n",
    "                          save_csv=True, random_state=0):\n",
    "    \"\"\"\n",
    "    Train a gradient-boosted tree for binary classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, y            : training data  (arrays)\n",
    "    n_iters         : int            number of boosting rounds\n",
    "    max_depth       : int            depth of each regression tree\n",
    "    learning_rate   : float          shrinkage coefficient Î·\n",
    "    save_csv        : bool           write AUC history to q5 data/ (default True)\n",
    "    random_state    : int            seed for reproducibility (kept optional)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model           : dict           fitted ensemble\n",
    "    auc_history     : list[float]    training AUC after each round\n",
    "    csv_path        : str | None     file written (None if save_csv=False)\n",
    "    \"\"\"\n",
    "    rng    = np.random.default_rng(random_state)\n",
    "    prior  = np.clip(y.mean(), 1e-8, 1-1e-8)\n",
    "    f      = np.full_like(y, np.log(prior / (1 - prior)), dtype=float)\n",
    "    trees, auc_hist = [], []\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        residuals = y - expit(f)                # pseudo-residuals\n",
    "        tree      = fit_gradient_tree(\n",
    "                        X, residuals,\n",
    "                        max_depth=max_depth,\n",
    "                        random_state=rng.integers(1e9))\n",
    "        trees.append(tree)\n",
    "        f += learning_rate * tree.predict(X)    # update logits\n",
    "        auc_hist.append(roc_auc_score(y, expit(f)))\n",
    "\n",
    "    model = {\"trees\": trees,\n",
    "             \"learning_rate\": learning_rate,\n",
    "             \"initial_score\": np.log(prior / (1 - prior))}\n",
    "\n",
    "    csv_path = None\n",
    "    if save_csv:\n",
    "        os.makedirs(\"q5 data\", exist_ok=True)\n",
    "        ts = dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        csv_path = f\"q5 data/auc_{ts}.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"iteration\": np.arange(1, n_iters + 1),\n",
    "            \"train_auc\": auc_hist\n",
    "        }).to_csv(csv_path, index=False)\n",
    "\n",
    "    return model, auc_hist, csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Gradient of cross-entropy objective\n",
    "\n",
    "Let's define:\n",
    "$p_{t,i} = \\sigma(f_t(x_i)) = \\frac{1}{1+e^{-f_t(x_i)}}$\n",
    "\n",
    "The loss at iteration $t$ is:\n",
    "$$L_t = -\\sum_{i=1}^{n} \\left[ y_i \\log p_{t,i} + (1-y_i)\\log(1-p_{t,i}) \\right]$$\n",
    "\n",
    "#### Step 1: Find derivative w.r.t. probability\n",
    "$$\\frac{\\partial L_t}{\\partial p_{t,i}} = -\\left(\\frac{y_i}{p_{t,i}} - \\frac{1-y_i}{1-p_{t,i}}\\right)$$\n",
    "\n",
    "#### Step 2: Find derivative of probability w.r.t. score\n",
    "$$\\frac{\\partial p_{t,i}}{\\partial f_{t,i}} = p_{t,i}(1-p_{t,i})$$\n",
    "\n",
    "#### Step 3: Apply chain rule\n",
    "$$\\frac{\\partial L_t}{\\partial f_{t,i}} = \\frac{\\partial L_t}{\\partial p_{t,i}} \\cdot \\frac{\\partial p_{t,i}}{\\partial f_{t,i}} = (p_{t,i}-y_i)$$\n",
    "\n",
    "#### Step 4: Pseudo-residual we fit in next iteration\n",
    "$$r_{t,i} = y_i - p_{t,i}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
