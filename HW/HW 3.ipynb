{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e",
   "metadata": {
    "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e"
   },
   "source": [
    "Welcome to Homework 3! The goal of this homework is to better understand generative models by implementing Probabilistic PCA on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b034214",
   "metadata": {},
   "source": [
    "Q1: Describe the model for PPCA in terms of the mean vector $\\mu$, the matrix $B$ describing the linear transformation from the latent data $z$ to the observed data $x$, and the variance $\\sigma^2$ of the isotropic Gaussian noise in the observed data. What are the model parameters that need to be estimated and what are their dimensions? When describing the probability model, be explicit regarding the dimensions of random variables/vectors. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03157a3",
   "metadata": {},
   "source": [
    "PPCA aims to identify a low-dimensional linear subspace (the loading matrix $B$), the mean $\\mu$ of the data, and the variance $\\sigma^2$ of the isotropic Gaussian noise in the observed data that maxamimze the likelihood of the observed data under a simple Gaussian latent variable model. In PPCA we’re doing a maximum‐likelihood fit of a Gaussian latent-variable model. \n",
    "\n",
    "In this way, PPCA provides a statistical foundation for dimensionality reduction by explicitly modeling the generative process of high-dimensional data. The model captures how low dimensional latent factors give rise to observed data through a linear transformation with uncertainty.\n",
    "\n",
    "The PPCA model posits that each observed data point arises from the following process:\n",
    "\n",
    "First, we consider a latent vector $z$ of dimension $m$, where $m$ is substantially smaller than the observed dimension $D$. This latent vector follows a standard Gaussian distribution in $m$-dimensional space, i.e. a cloud of probability centered at the origin with unit variance in all directions.\n",
    "\n",
    "Next, this latent representation undergoes a linear transformation through the loading matrix $B$ (dimensions $D \\times m$), which maps the low-dimensional latent space to the higher-dimensional observed space. This transformation captures the principal directions of variation in the data.\n",
    "\n",
    "The transformed latent vector is then shifted by the mean vector $\\mu$ (dimension $D$), representing the central tendency of the observed data distribution.\n",
    "\n",
    "Finally, to account for measurement noise and variation not captured by the $m$ principal components, isotropic Gaussian noise with variance $\\sigma^2$ is added independently to each dimension of the observed space.\n",
    "\n",
    "This results in a conditional Gaussian distribution for the observed data given the latent variables, with mean $\\mu + Bz$ and covariance $\\sigma^2I_D$. When we marginalize out the latent variables, the observed data follows a Gaussian distribution with mean $\\mu$ and covariance $BB^T + \\sigma^2I_D$, decomposing the total variance into structured variation (captured by $BB^T$) and unstructured noise ($\\sigma^2I_D$).\n",
    "\n",
    "The parameters requiring estimation are:\n",
    "\n",
    "* The mean vector $\\mu$ (dimension $D$), represents the center of the data cloud\n",
    "* The loading matrix $B$ (dimension $D \\times m$), defines the principal subspace\n",
    "* The noise variance $\\sigma^2$ (a positive scalar), quantifies the residual variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d2f60",
   "metadata": {},
   "source": [
    "Q2: Based on your description, implement a function for generating `n` observations from a PPCA model, which takes in as arguments the mean vector $\\mu$, the matrix $B$, the variance $\\sigma^2$ of the isotropic Gaussian noise in the observed data, and the number of observations $n$. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151125f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_ppca(mu: np.ndarray, B: np.ndarray, sigma2: float, n: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate n observations from a PPCA model:\n",
    "        z ~ N(0, I_q)\n",
    "        x = mu + B z + eps,   eps ~ N(0, sigma2 I_D)\n",
    "    \n",
    "    Args:\n",
    "        mu     : np.ndarray of shape (D,) or (D,1) - the data mean\n",
    "        B      : np.ndarray of shape (D, q) - the loading matrix\n",
    "        sigma2 : float - noise variance\n",
    "        n      : int - number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        X : np.ndarray of shape (n, D) - generated data points\n",
    "    \"\"\"\n",
    "    mu = np.asarray(mu).reshape(1, -1)     # (1, D)\n",
    "    B = np.asarray(B)                      # (D, q)\n",
    "    D, q = B.shape\n",
    "\n",
    "    # --- draw latent z’s ---\n",
    "    Z = np.random.randn(n, q)              # (n, q)\n",
    "\n",
    "    # --- map to data space ---\n",
    "    XB = Z.dot(B.T)                        # (n, D)\n",
    "\n",
    "    # --- add mean ---\n",
    "    XB += mu                               # broadcast to (n, D)\n",
    "\n",
    "    # --- add isotropic noise ---\n",
    "    noise = np.random.randn(n, D) * np.sqrt(sigma2)\n",
    "    X = XB + noise\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00702b0b",
   "metadata": {},
   "source": [
    "Q3: What is the closed form solution for PPCA from maximum likelihood estimation? Provide the equations. Be explicit with all dimensions. Make sure all notation used is defined. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb7d03",
   "metadata": {},
   "source": [
    "\n",
    "**Assumptions**\n",
    "\n",
    "* Observed data matrix $X \\in \\mathbb{R}^{n \\times D}$ has **rows = observations** and **columns = features**.  \n",
    "* Latent dimensionality $q < D$.  \n",
    "* Generative model:  \n",
    "  * Latent code $z \\in \\mathbb{R}^{q}$ with prior $p(z)=\\mathcal{N}(0,I_q)$.  \n",
    "  * Observation $x \\mid z \\sim \\mathcal{N}(\\mu + Bz, \\sigma^{2} I_D)$.  \n",
    "    * $\\mu \\in \\mathbb{R}^{D}$ — mean of the data.  \n",
    "    * $B \\in \\mathbb{R}^{D \\times q}$ — loading (linear-map) matrix.  \n",
    "    * $\\sigma^{2}\\in\\mathbb{R}_{>0}$ — isotropic noise variance.  \n",
    "\n",
    "---\n",
    "    \n",
    "  * **Parameters**:\n",
    "    * $\\mu \\in \\mathbb{R}^{D}$ — global mean vector with $D$ elements (one per feature)\n",
    "    * $B \\in \\mathbb{R}^{D \\times q}$ — loading matrix that maps each latent dimension to its effect on each observed dimension\n",
    "      * Each column of $B$ represents how one latent factor influences all $D$ observed variables\n",
    "      * Each row of $B$ represents how all $q$ latent factors influence one observed variable\n",
    "    * $\\sigma^{2} \\in \\mathbb{R}_{>0}$ — positive scalar representing the variance of the observation noise in each dimension\n",
    "      * Isotropic defined as equal variance in all directions (spherical noise)\n",
    "---\n",
    "**Notation and variables:**\n",
    "- $X \\in \\mathbb{R}^{n \\times D}$: data matrix with $n$ observations and $D$ dimensions\n",
    "- $x_n \\in \\mathbb{R}^D$: the $n$-th data point (row of $X$, written as a column vector)\n",
    "- $q < D$: chosen dimensionality of the latent space/factors\n",
    "- $\\hat{\\mu} \\in \\mathbb{R}^D$: estimated mean vector\n",
    "- $x_n^c \\in \\mathbb{R}^D$: centered data point ($x_n - \\hat{\\mu}$)\n",
    "- $S \\in \\mathbb{R}^{D \\times D}$: sample covariance matrix\n",
    "- $U \\in \\mathbb{R}^{D \\times D}$: orthogonal matrix of eigenvectors of $S$\n",
    "- $\\Lambda \\in \\mathbb{R}^{D \\times D}$: diagonal matrix of eigenvalues of $S$\n",
    "- $\\lambda_j \\in \\mathbb{R}$: $j$-th eigenvalue of $S$ (in descending order)\n",
    "- $U_q \\in \\mathbb{R}^{D \\times q}$: first $q$ columns of $U$ (principal eigenvectors)\n",
    "- $\\Lambda_q \\in \\mathbb{R}^{q \\times q}$: diagonal matrix of first $q$ eigenvalues\n",
    "- $\\hat{\\sigma}^2 \\in \\mathbb{R}_{>0}$: estimated isotropic noise variance\n",
    "- $I_q \\in \\mathbb{R}^{q \\times q}$: $q \\times q$ identity matrix\n",
    "- $R \\in \\mathbb{R}^{q \\times q}$: arbitrary orthogonal rotation matrix\n",
    "- $\\hat{B} \\in \\mathbb{R}^{D \\times q}$: estimated loading matrix\n",
    "---\n",
    "\n",
    "**Closed Form Solution**\n",
    "\n",
    "1. **Data mean**  \n",
    "   $\\hat{\\mu} = \\frac{1}{n}\\sum_{n=1}^n x_n \\in \\mathbb{R}^D$,  \n",
    "   $x_n^c = x_n - \\hat{\\mu}$\n",
    "\n",
    "2. **Sample covariance and eigen-decomposition**  \n",
    "   $S = \\frac{1}{n}\\sum_{n=1}^n x_n^c (x_n^c)^{\\top} = U\\Lambda U^{\\top}$,  \n",
    "   \n",
    "   where $U \\in \\mathbb{R}^{D \\times D}$ is orthogonal and $\\Lambda = \\text{diag}(\\lambda_1,\\dots,\\lambda_D)$ with $\\lambda_1 \\geq \\cdots \\geq \\lambda_D$.\n",
    "\n",
    "3. **Noise variance**  \n",
    "   $\\hat{\\sigma}^2 = \\frac{1}{D-q}\\sum_{j=q+1}^{D}\\lambda_j$, $\\hat{\\sigma}^2 \\in \\mathbb{R}$\n",
    "\n",
    "4. **Loading matrix**  \n",
    "   Let $U_q = [u_1,\\dots,u_q] \\in \\mathbb{R}^{D \\times q}$,  \n",
    "   $\\Lambda_q = \\text{diag}(\\lambda_1,\\dots,\\lambda_q) \\in \\mathbb{R}^{q \\times q}$  \n",
    "   \n",
    "   For any orthogonal $R \\in \\mathbb{R}^{q \\times q}$,  \n",
    "   $\\hat{B} = U_q (\\Lambda_q - \\hat{\\sigma}^2 I_q)^{1/2} R$, $\\hat{B} \\in \\mathbb{R}^{D \\times q}$  \n",
    "   \n",
    "   A common choice is $R = I_q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1488f",
   "metadata": {},
   "source": [
    "Q4: Implement a function that performs maximum likelihood estimation for PPCA. The function should take in the observed data and the dimension of the latent factors $m$. It should return $\\hat{\\mu}, \\hat{B},$ and $\\hat{\\sigma}^2$. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae91a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "def ppca_mle(X: np.ndarray, m: int) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Maximum–likelihood estimation for Probabilistic PCA (Tipping & Bishop, 1999).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, D)\n",
    "        Observed data matrix. Rows are samples, columns are observed dimensions.\n",
    "    m : int\n",
    "        Latent dimensionality (number of principal components to retain).\n",
    "        Must satisfy 1 ≤ m < D.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mu_hat : ndarray of shape (D,)\n",
    "        Estimated data mean vector.\n",
    "    B_hat : ndarray of shape (D, m)\n",
    "        Estimated loading matrix (sometimes denoted W).\n",
    "    sigma2_hat : float\n",
    "        Estimated isotropic noise variance.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The solution maximises the likelihood under the PPCA model\n",
    "\n",
    "        z  ~  N(0, I_m)\n",
    "        x | z  ~  N(mu + B z, σ² I_D).\n",
    "\n",
    "    The closed-form ML estimates are:\n",
    "\n",
    "        μ̂      = sample mean  \n",
    "        σ̂²     = (1/(D-m)) Σ_{j=m+1}^D λ_j  \n",
    "        B̂      = U_m (Λ_m – σ̂² I)^{1/2} R\n",
    "\n",
    "    where {λ_j, U_j} are the eigenpairs of the sample covariance matrix S,\n",
    "    Λ_m = diag(λ_1, …, λ_m), and R is any orthogonal matrix (we take R = I).\n",
    "    \"\"\"\n",
    "    # ---------- basic checks ----------\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be a 2-D array (n_samples × D).\")\n",
    "    n, D = X.shape\n",
    "    if not 1 <= m < D:\n",
    "        raise ValueError(f\"m must satisfy 1 ≤ m < D (= {D}).\")\n",
    "\n",
    "    # ---------- centre data ----------\n",
    "    mu_hat = X.mean(axis=0)\n",
    "    X_centered = X - mu_hat\n",
    "\n",
    "    # ---------- sample covariance ----------\n",
    "    # using (1/n) rather than (1/(n-1)) because we are in an ML setting\n",
    "    S = (X_centered.T @ X_centered) / n      # shape: (D, D)\n",
    "\n",
    "    # ---------- eigen-decomposition ----------\n",
    "    # eigh returns ascending order; reverse for descending\n",
    "    eigvals, eigvecs = eigh(S)\n",
    "    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n",
    "\n",
    "    # ---------- noise variance ----------\n",
    "    sigma2_hat = float(np.mean(eigvals[m:]))   # eq. (12) in Tipping & Bishop\n",
    "\n",
    "    # ---------- loading matrix ----------\n",
    "    # choose R = I (any orthogonal rotation is ML-equivalent)\n",
    "    Lambda_m = np.diag(np.sqrt(np.maximum(eigvals[:m] - sigma2_hat, 0.0)))\n",
    "    B_hat = eigvecs[:, :m] @ Lambda_m          # eq. (11)\n",
    "\n",
    "    return mu_hat, B_hat, sigma2_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabe287",
   "metadata": {},
   "source": [
    "Q5: Test out your implementation of PPCA. Generate data using your function from Q2. Generate $n=1000$ observations with $m=2$ latent dimensions, $p=4$ observed dimensions, $\\mu=\\vec{0}_4$, $B = np.array([[1,1],[0,1],[1,0],[1,-1]])$, and $\\sigma^2 = 0.25$. What are your estimates for the model parameters? How close are your estimated parameters for $\\mu$ and $\\sigma^2$ to the truth? To check your estimate for $B$, check that $\\hat{B} \\hat{B}^\\top$ is close to $B B^\\top$. (Hint: Your solution should be pretty close.) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d236e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_hat: [ 0.10294883  0.03488957 -0.00653038 -0.02921555]\n",
      "mu: [0. 0. 0. 0.]\n",
      "sigma2_hat: 0.2460923471957588\n",
      "sigma: 0.25\n",
      "BBT_hat: [[ 1.99415744  1.03044633  0.95980023 -0.10536937]\n",
      " [ 1.03044633  1.04454757  0.00601647 -1.07027492]\n",
      " [ 0.95980023  0.00601647  0.93071995  0.92119547]\n",
      " [-0.10536937 -1.07027492  0.92119547  2.02068277]]\n",
      "BBT_true: [[ 2  1  1  0]\n",
      " [ 1  1  0 -1]\n",
      " [ 1  0  1  1]\n",
      " [ 0 -1  1  2]]\n",
      "Mean error: 0.112747\n",
      "BBT relative error: 0.056227\n",
      "Noise variance relative error: 0.015631\n",
      "\n",
      "Validation results:\n",
      "Mean estimation: FAIL\n",
      "Covariance structure: PASS\n",
      "Noise variance: PASS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Set seed for reproducibility ---\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Define parameters ---\n",
    "mu = np.zeros(4)\n",
    "B = np.array([[1, 1],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, -1]])\n",
    "sigma = 0.25 \n",
    "n = 1000\n",
    "\n",
    "# --- Generate data ---\n",
    "\n",
    "X = generate_ppca(mu = mu, B = B, sigma2 = sigma, n = n)\n",
    "\n",
    "# --- Fit PPCA ---\n",
    "\n",
    "mu_hat, B_hat, sigma2_hat = ppca_mle(X, m = 2)\n",
    "\n",
    "# --- Compute estimates ---\n",
    "\n",
    "BBT_hat = B_hat @ B_hat.T\n",
    "BBT_true = B @ B.T\n",
    "\n",
    "print(\"mu_hat:\", mu_hat)\n",
    "print(\"mu:\", mu)\n",
    "print(\"sigma2_hat:\", sigma2_hat)\n",
    "print(\"sigma:\", sigma)\n",
    "print(\"BBT_hat:\", BBT_hat)\n",
    "print(\"BBT_true:\", BBT_true)\n",
    "\n",
    "# --- Check estimates ---\n",
    "mu_error = np.linalg.norm(mu_hat) if np.allclose(mu, 0) else np.linalg.norm(mu_hat - mu) / np.linalg.norm(mu)\n",
    "print(f\"Mean error: {mu_error:.6f}\")\n",
    "BBT_rel_error = np.linalg.norm(BBT_hat - BBT_true, 'fro') / np.linalg.norm(BBT_true, 'fro')\n",
    "print(f\"BBT relative error: {BBT_rel_error:.6f}\")\n",
    "sigma2_rel_error = abs(sigma2_hat - sigma) / sigma\n",
    "print(f\"Noise variance relative error: {sigma2_rel_error:.6f}\")\n",
    "\n",
    "# --- Validation ---\n",
    "print(\"\\nValidation results:\")\n",
    "print(f\"Mean estimation: {'PASS' if mu_error < 0.10 else 'FAIL'}\")\n",
    "print(f\"Covariance structure: {'PASS' if BBT_rel_error < 0.10 else 'FAIL'}\")\n",
    "print(f\"Noise variance: {'PASS' if sigma2_rel_error < 0.10 else 'FAIL'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cce15",
   "metadata": {},
   "source": [
    "Q6: We will now apply PPCA to the MNIST digit dataset, but only for images of the digit zero. Import and filter the MNIST digit dataset to only images of the digit zero using the following code. Then use the `visualize_digits` function to visualize the first four digits in this dataset. (1 point)\n",
    "```\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# filter for digit zero\n",
    "zero_idxs = digits.target == 0\n",
    "mnist0_imgs = digits.images[zero_idxs]\n",
    "\n",
    "# flatten the images into vectors\n",
    "mnist0_X = mnist0_imgs.reshape((len(mnist0_imgs), -1))\n",
    "\n",
    "def visualize_digits(mnist_flattened_X):\n",
    "    # This function will visualize the first 4 digits in the provided data\n",
    "    _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "    for ax, image in zip(axes, mnist_flattened_X):\n",
    "        ax.set_axis_off()\n",
    "        image = image.reshape(8, 8)\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c652c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# --- Load the data ---\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# --- Filter for digit zero ---\n",
    "zero_idxs = digits.target == 0\n",
    "mnist0_imgs = digits.images[zero_idxs]\n",
    "\n",
    "# --- Flatten the images into vectors ---\n",
    "mnist0_X = mnist0_imgs.reshape((len(mnist0_imgs), -1))\n",
    "\n",
    "def visualize_digits(mnist_flattened_X):\n",
    "    # This function will visualize the first 4 digits in the provided data\n",
    "    _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "    for ax, image in zip(axes, mnist_flattened_X):\n",
    "        ax.set_axis_off()\n",
    "        image = image.reshape(28, 28)\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e47ae",
   "metadata": {},
   "source": [
    "Q7: Apply your PPCA model fitting code to the MNIST dataset, restricting to only those images of the digit zero. Use latent dimension 3 to fit the PPCA model. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b70c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ̂ shape: (784,)\n",
      "B̂ shape: (784, 3)\n",
      "σ̂²: 0.0396\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# --- Load and filter MNIST zeros ---\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data.astype(float)[mnist.target.astype(int) == 0] / 255.0  # (n_zeros, 784)\n",
    "\n",
    "# --- Fit PPCA (latent dim = 3) ---\n",
    "mu_hat, B_hat, sigma2_hat = ppca_mle(X, 3)\n",
    "\n",
    "# --- Report shapes & noise variance ---\n",
    "print(f\"μ̂ shape: {mu_hat.shape}\")\n",
    "print(f\"B̂ shape: {B_hat.shape}\")\n",
    "print(f\"σ̂²: {sigma2_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2784",
   "metadata": {},
   "source": [
    "Q8: Now generate four images of the digit zero using your fitted model (use the function you implemented in Q2)! Visualize your generated images using the helper function `visualize_digits`. Note that you might need to cast the generated data to float and truncate any generated pixel values smaller than zero. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bd1110c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Generate 4 new “0” images from the fitted PPCA model ---\n",
    "X_gen = generate_ppca(mu_hat, B_hat, sigma2_hat, n=4).astype(float)\n",
    "\n",
    "# --- Truncate negatives (pixels can’t go below 0) ---\n",
    "X_gen = np.clip(X_gen, 0.0, None)\n",
    "\n",
    "# --- Visualize with the provided helper (expects shape (n, 784)) ---\n",
    "visualize_digits(X_gen)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
