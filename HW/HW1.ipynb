{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b19e4fe2",
      "metadata": {},
      "source": [
        "Welcome to Homework 1! Your task for this homework is to implement logistic regression from scratch. You are welcome to use existing software packages to check your work. However, implementing it once in your life will help you better understand how this algorithm works.\n",
        "\n",
        "Recall: Logistic regression is a classification model that estimates the probability of a binary outcome $Y$ being equal to 1, given variables/features $X$. It assumes that the log odds is linear with respect to $X$. Because it can be viewed as a generalization of linear regression, it falls under the general umbrella of methods called \"generalizaed linear models.\"\n",
        "\n",
        "Helpful resources:\n",
        "* https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "\n",
        "Concepts you'll need from lectures:\n",
        "* Maximum likelihood estimation\n",
        "* Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264cfd00-ec55-45ce-b999-3c86a475d348",
      "metadata": {
        "id": "264cfd00-ec55-45ce-b999-3c86a475d348"
      },
      "source": [
        "**How does Logistic Regression work?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecad7236",
      "metadata": {
        "id": "ecad7236"
      },
      "source": [
        "Q1: What is the mathematical equation that describes the probability distribution of a binary random variable? (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590b8e94",
      "metadata": {},
      "source": [
        "$p(x; q) = q^x(1-q)^{1-x}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5562197",
      "metadata": {
        "id": "f5562197"
      },
      "source": [
        "Q2: What probability distribution does logistic regression assume $Y|X$ follows? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170e5ae0",
      "metadata": {},
      "source": [
        "Bernoulli  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a64e62b",
      "metadata": {
        "id": "4a64e62b"
      },
      "source": [
        "Q3: What are the parameters of a logistic regression model? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0710ae8",
      "metadata": {},
      "source": [
        "\n",
        "The parameters of a logistic regression model are the regression coefficients $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)$, where:\n",
        "\n",
        "- $\\beta_0$ is the intercept term \n",
        "- $\\beta_1, \\beta_2, \\ldots, \\beta_p$ are the coefficients for each of the $p$ features/predictors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0761a7c9",
      "metadata": {
        "id": "0761a7c9"
      },
      "source": [
        "Q4: What is the log likelihood of the parameters given observations $(X_i, Y_i)$ for $i=1,\\cdots, n$? (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db08843",
      "metadata": {},
      "source": [
        "Using the log likelihood for N observations:\n",
        "$$\\ell(\\beta) = \\sum_{i=1}^{N} \\log p_{g_i}(x_i; \\beta)$$\n",
        "\n",
        "Given observations $(X_i, Y_i)$ for $i=1,\\cdots, n$ the log likelihood for binary logistic regression parameters $\\beta$ is defined as:$$\\ell(\\beta) = \\sum_{i=1}^{N} \\left[ y_i \\log p(x_i;\\beta) + (1-y_i) \\log(1-p(x_i;\\beta)) \\right]$$\n",
        "$$=\\ell(\\beta) = \\sum_{i=1}^{    N} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "* $g_i = y_i \\in \\{0,1\\}$\n",
        "\n",
        "* $p(x_i;\\beta) = \\frac{1}{1 + e^{-\\beta^T x_i}}$, the probability $Y_i = 1$ given $X_i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e2aba8",
      "metadata": {
        "id": "b5e2aba8"
      },
      "source": [
        "Q5: What is the optimization problem that we try to solve when fitting logistic regression?  (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "In theory, we try to solve the problem of finding $\\beta$ that maximizes the log-likelihood function for logistic regression\n",
        "\n",
        "$$\n",
        "\\hat{\\beta} = \\arg\\max_{\\beta \\in \\mathbb{R}^p} \\ \\ell(\\beta)\n",
        "$$\n",
        "\n",
        "Where $\\ell(\\beta)$ is the log-likelihood function:\n",
        "$$ \\hat{\\beta} = \\sum_{i=1}^{N} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "The probability $p(x_i; \\beta)$ is given by the logistic function:\n",
        "\n",
        "$$\n",
        "p(x_i; \\beta) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta^T x_i)}}\n",
        "$$\n",
        "\n",
        "\n",
        "And:\n",
        "- $y_i$ is the observed class (0 or 1) for observation $i$\n",
        "- $p(x_i;\\beta) = \\frac{1}{1 + e^{-\\beta^T x_i}}$ is the predicted probability that $y_i=1$ for observation $i$\n",
        "- $\\beta$ represents the model parameters (coefficients)\n",
        "\n",
        "In practice, we aim to minimize the negative log-likelihood, or minimimze the cross-entropy loss or log loss, finding the paramater values for a gradient = 0:\n",
        "\n",
        "$$\\hat{\\beta} = \\arg\\min_{\\beta} -\\ell(\\beta)$$\n",
        "\n",
        "$$\\hat{\\beta} = \\arg\\min_{\\beta} \\sum_{i=1}^{N} \\left[-y_i \\beta^T x_i + \\log(1 + e^{\\beta^T x_i})\\right]$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6990b0d",
      "metadata": {
        "id": "c6990b0d"
      },
      "source": [
        "Q6: What procedures can be used to solve the optimization problem underlying logistic regression? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02137cd3",
      "metadata": {},
      "source": [
        " To maximize the log-likelihood, set the gradient(vector of derivatives) to zero forming the score equations\n",
        "$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = \\sum_{i=1}^{N} x_i(y_i - p(x_i; \\beta)) = 0$$\n",
        "Solve the score equations simplified Newton–Raphson algorithm\n",
        "   \n",
        "   * will need second derivative\n",
        "  \n",
        "   * will need Hessian matrix \n",
        "\n",
        "\n",
        "In matrix notation: score equations and Hessian in matrix notation\n",
        "\n",
        "$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p})$$\n",
        "\n",
        "$$\\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta^T} = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$$\n",
        "\n",
        "Newton step:\n",
        "\n",
        "$$\\beta^{\\text{new}} = \\beta^{\\text{old}} + (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\\mathbf{X}^T (\\mathbf{y} - \\mathbf{p})$$\n",
        "\n",
        "$$= (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{W} \\left( \\mathbf{X}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y} - \\mathbf{p}) \\right)$$\n",
        "\n",
        " Newton step as a weighted least squares step:\n",
        "\n",
        "$$\\mathbf{z} = \\mathbf{X}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y} - \\mathbf{p})$$\n",
        "\n",
        "* $\\mathbf{y}$ denotes the vector of $y_i$ values (observed outcomes)\n",
        "* $\\mathbf{X}$ is the $N \\times (p + 1)$ matrix of $x_i$ values (feature vectors)\n",
        "* $\\mathbf{p}$ is the vector of fitted probabilities where:\n",
        "  * The ith element is $p(x_i;\\beta^{\\text{old}})$\n",
        "* $\\mathbf{W}$ is a $N \\times N$ diagonal matrix of weights where:\n",
        "  * The ith diagonal element is $p(x_i;\\beta^{\\text{old}})(1 - p(x_i; \\beta^{\\text{old}}))$\n",
        "\n",
        "\n",
        " Update variables, implement solving reweighted least squares  (IRLS algorithm) at each iteration p changes (IRLS algorithm)\n",
        "\n",
        "$$\\beta^{new} = \\arg\\min_{\\beta} (z - X\\beta)^T W(z - X\\beta)$$\n",
        "\n",
        "Stop when $\\boldsymbol{\\beta}_{\\text{new}}$ and $\\boldsymbol{\\beta}_{\\text{old}}$ are very close (converged)\n",
        "\n",
        "\n",
        "Alternatively, you can implement gradient descent, an iterative algorithm that updates parameters in the direction of the negative gradient of the cost function.\n",
        "   \n",
        "   $\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_\\beta J(\\beta^{(t)})$\n",
        "   \n",
        "   where $\\alpha$ is the learning rate and $\\nabla_\\beta J(\\beta)$ is the gradient of the cost function with respect to $\\beta$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20844750",
      "metadata": {},
      "source": [
        "Q6 Part 2: What procedures can be used to solve the optimization problem underlying logistic regression? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. Compute $\\mathbf{p}$ from $\\boldsymbol{\\beta}_{\\text{old}}$:\n",
        "   $$p_i = \\frac{1}{1 + e^{-\\boldsymbol{\\beta}_{\\text{old}}^T \\mathbf{x}_i}}$$\n",
        "\n",
        "2. Build $\\mathbf{W}$:\n",
        "   - Diagonal entries: $p_i (1 - p_i)$\n",
        "\n",
        "3. Build the adjusted response $\\mathbf{z}$:\n",
        "   $$\\mathbf{z} = \\mathbf{X} \\boldsymbol{\\beta}_{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y} - \\mathbf{p})$$\n",
        "\n",
        "\n",
        "4. Solve a weighted least squares problem:\n",
        "   \n",
        "   Minimize:\n",
        "   $$(\\mathbf{z} - \\mathbf{X} \\boldsymbol{\\beta})^T \\mathbf{W} (\\mathbf{z} - \\mathbf{X} \\boldsymbol{\\beta})$$\n",
        "   \n",
        "   The solution for the best $\\boldsymbol{\\beta}_{\\text{new}}$ is:\n",
        "   $$\\boldsymbol{\\beta}_{\\text{new}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{z}$$\n",
        "\n",
        "5. Update $\\boldsymbol{\\beta}$:\n",
        "   Set $\\boldsymbol{\\beta}_{\\text{old}} \\leftarrow \\boldsymbol{\\beta}_{\\text{new}}$ and repeat.\n",
        "\n",
        "As you update, $\\mathbf{p}$, $\\mathbf{W}$, and $\\mathbf{z}$ all change because they depend on the current $\\boldsymbol{\\beta}$.\n",
        "\n",
        "Stop when $\\boldsymbol{\\beta}_{\\text{new}}$ and $\\boldsymbol{\\beta}_{\\text{old}}$ are very close (converged)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cd6622",
      "metadata": {},
      "source": [
        "Question 6 Part 3: What procedures can be used to solve the optimization problem underlying logistic regression? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd888c46",
      "metadata": {},
      "source": [
        "Using Newton's method per ESLII textbook requires using the second derivative and is more computationally expensive. The gradient descent update rule is:\n",
        "\n",
        "$$\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\alpha \\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = \\beta^{\\text{old}} - \\alpha X^T (y - p)$$\n",
        "\n",
        "* $\\alpha$ is the learning rate\n",
        "\n",
        "\n",
        "The Newton update rule is:\n",
        "\n",
        "   $$\\boldsymbol{\\beta}_{\\text{new}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{z}$$\n",
        "\n",
        "   * $z$ is the adjusted response\n",
        "\n",
        "Therefore, the gradient descent is a better procedure to solve the optimization problem underlying logistic regression.\n",
        "\n",
        "\n",
        "1. Initialize $\\boldsymbol{\\beta}^{(0)}$:\n",
        "\n",
        "2. At each iteration $t$:\n",
        "\n",
        "   2.1. Compute the predicted probabilities $\\mathbf{p}^{(t)}$ from current $\\boldsymbol{\\beta}^{(t)}$:\n",
        "   \n",
        "   $$p_i^{(t)} = \\frac{1}{1 + e^{-\\boldsymbol{\\beta}^{(t)T} \\mathbf{x}_i}}$$\n",
        "   \n",
        "   2.2. Compute the gradient of the cost function $J(\\boldsymbol{\\beta})$:\n",
        "   \n",
        "   $$\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}^{(t)}) = -\\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}^{(t)})$$\n",
        "   \n",
        "   2.3. Update $\\boldsymbol{\\beta}$ using gradient descent:\n",
        "   \n",
        "   $$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\alpha \\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}^{(t)})$$\n",
        "   \n",
        "   * $\\alpha > 0$ is the learning rate\n",
        "\n",
        "3. Repeat steps 2.1 to 2.3 until convergence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d73b5d",
      "metadata": {
        "id": "81d73b5d"
      },
      "source": [
        "Q7: Derive the gradient of the log likelihood with respect to the parameters of the logistic regression model step by step. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2685ea3",
      "metadata": {},
      "source": [
        "Let the log likelihood with respect to the parameters of the logistic regression model be:\n",
        "$$\\ell(\\beta) = \\sum_{i=1}^{N} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "\n",
        "\n",
        "partial derivatives with respect to each parameter $\\beta_j$:\n",
        "\n",
        "$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^{N} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "$$= \\sum_{i=1}^{N} \\frac{\\partial}{\\partial \\beta_j} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "for the first term:\n",
        "$$\\frac{\\partial}{\\partial \\beta_j} (y_i \\beta^T x_i) = y_i x_{ij}$$\n",
        "\n",
        "for the second term, using the chain rule:\n",
        "$$\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\beta^T x_i}) = \\frac{1}{1+e^{\\beta^T x_i}} \\cdot e^{\\beta^T x_i} \\cdot x_{ij} = \\frac{e^{\\beta^T x_i}}{1+e^{\\beta^T x_i}} \\cdot x_{ij}$$\n",
        "\n",
        "Where $\\frac{e^{\\beta^T x_i}}{1+e^{\\beta^T x_i}} = p(x_i;\\beta)$, is the predicted probability that $y_i = 1$.\n",
        "\n",
        "Combining these terms:\n",
        "$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^{N} \\left[ y_i x_{ij} - p(x_i;\\beta) x_{ij} \\right] = \\sum_{i=1}^{N} \\left[ y_i - p(x_i;\\beta) \\right] x_{ij}$$\n",
        "\n",
        "Writing in matrix notation: \n",
        "$$\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = X^T (y - p)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\mathbf{y}$ denotes the vector of $y_i$ values design matrix with rows $x_i^T$ (observed outcomes)\n",
        "* $\\mathbf{X}$ is the $N \\times (p + 1)$ matrix of $x_i$ values (feature vectors)\n",
        "* $\\mathbf{p}$ is the vector of fitted probabilities where:\n",
        "  * The ith element is $p(x_i;\\beta^{\\text{old}})$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ff04c8",
      "metadata": {},
      "source": [
        "### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f",
      "metadata": {
        "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f"
      },
      "source": [
        "**Implement Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008739de",
      "metadata": {
        "id": "008739de"
      },
      "source": [
        "Q1: Write the function `generate_X(n,p)`, which returns randomly generated $X_1,\\cdots, X_n$, where $X_i \\in \\mathbb{R}^p$. You can sample the variables using a uniform distribution or a standard normal distribution. (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2b6a3f51",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_X(n, p):\n",
        "    \"\"\"\n",
        "    Generate a random feature matrix X with n samples and p features.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n : int\n",
        "        Number of samples to generate\n",
        "    p : int\n",
        "        Number of features for each sample\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X : numpy.ndarray\n",
        "        Matrix of shape (n, p) containing the generated features\n",
        "    \"\"\"\n",
        "    # Using standard normal distribution (mean=0, std=1)\n",
        "    X = np.random.normal(size=(n, p))\n",
        "    \n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4598cf",
      "metadata": {
        "id": "cb4598cf"
      },
      "source": [
        "Q2: Write the function `generate_Y(X, beta, intercept)`, which generates outcomes for observations $X_1,\\cdots, X_p$ per a logistic regression model with coefficients $\\beta \\in \\mathbb{R}^{p}$ and intercept $\\beta_0$. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a547fbf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_Y(X, beta, intercept):\n",
        "    \"\"\"\n",
        "    Generate binary outcomes Y based on a logistic regression model.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix of shape (n, p) where n is the number of samples\n",
        "        and p is the number of features\n",
        "    beta : numpy.ndarray\n",
        "        Coefficient vector of shape (p,) for the features\n",
        "    intercept : float\n",
        "        Intercept term (β₀) in the logistic regression model\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    Y : numpy.ndarray\n",
        "        Binary outcome vector of shape (n,) containing 0s and 1s\n",
        "    \"\"\"\n",
        "    # Number of samples\n",
        "    n = X.shape[0]\n",
        "    \n",
        "    # Calculate the linear predictor: β₀ + X·β\n",
        "    z = intercept + np.dot(X, beta)\n",
        "    \n",
        "    # Apply the logistic function to get probabilities\n",
        "    # p(x) = 1 / (1 + exp(-z))\n",
        "    probabilities = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    # Generate binary outcomes by comparing probabilities to random uniform values\n",
        "    # Simulates Bernoulli trials with the calculated probabilities\n",
        "    random_values = np.random.uniform(0, 1, n)\n",
        "    Y = (probabilities > random_values).astype(int)\n",
        "    \n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e66ccc4",
      "metadata": {
        "id": "8e66ccc4"
      },
      "source": [
        "Q3: Generate some data using your functions above with $p=2$, $n=1000$, coefficients $\\beta=(0.5,2)$, and intercept $\\beta_0 = 1$. (7 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "50492959",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters\n",
        "n = 1000  # number of samples\n",
        "p = 2     # number of features\n",
        "beta = np.array([0.5, 2])  # coefficients\n",
        "intercept = 1  # intercept \n",
        "\n",
        "# Generate features X\n",
        "X = generate_X(n, p)\n",
        "\n",
        "# Generate binary outcomes Y\n",
        "Y = generate_Y(X, beta, intercept)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5a40b8",
      "metadata": {
        "id": "0c5a40b8"
      },
      "source": [
        "Q4: Implement a function that runs gradient descent `run_gradient_descent(X, Y, alpha, num_iterations, initial_betas)`. Make sure to vectorize your code. (Otherwise it will run really slowly.) (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bf9f63bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def run_gradient_descent(X, Y, alpha, num_iterations, initial_betas):\n",
        "    \"\"\"\n",
        "    Run gradient descent to find optimal parameters for logistic regression.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix of shape (n, p) where n is the number of samples\n",
        "        and p is the number of features (without intercept)\n",
        "    Y : numpy.ndarray\n",
        "        Binary outcome vector of shape (n,) containing 0s and 1s\n",
        "    alpha : float\n",
        "        Learning rate for gradient descent\n",
        "    num_iterations : int\n",
        "        Number of iterations to run gradient descent\n",
        "    initial_betas : numpy.ndarray\n",
        "        Initial values for all parameters (including intercept) of shape (p+1,)\n",
        "        where initial_betas[0] is the intercept\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    betas : numpy.ndarray\n",
        "        Optimized parameters after gradient descent\n",
        "    cost_history : list\n",
        "        List of cost values at each iteration\n",
        "    \"\"\"\n",
        "    # Number of samples\n",
        "    n = X.shape[0]\n",
        "    \n",
        "    # Add a column of 1s to X for the intercept term\n",
        "    X_with_intercept = np.column_stack((np.ones(n), X))\n",
        "    \n",
        "    # Initialize parameters and cost history\n",
        "    betas = initial_betas.copy()\n",
        "    cost_history = []\n",
        "    \n",
        "    # Run gradient descent for specified number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        # Calculate the linear predictor z = X·β\n",
        "        z = np.dot(X_with_intercept, betas)\n",
        "        \n",
        "        # Calculate predicted probabilities using the logistic function\n",
        "        predictions = 1 / (1 + np.exp(-z))\n",
        "        \n",
        "        # Calculate the cost (negative log likelihood)\n",
        "        # Using a numerically stable version to avoid log(0) issues\n",
        "        epsilon = 1e-15  # Small value to avoid log(0)\n",
        "        predictions_safe = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "        cost = -np.mean(Y * np.log(predictions_safe) + (1 - Y) * np.log(1 - predictions_safe))\n",
        "        cost_history.append(cost)\n",
        "        \n",
        "        # Calculate the gradient (vectorized)\n",
        "        errors = predictions - Y\n",
        "        gradient = np.dot(X_with_intercept.T, errors) / n\n",
        "        \n",
        "        # Update parameters using gradient descent\n",
        "        betas = betas - alpha * gradient\n",
        "        \n",
        "    return betas, cost_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62965710",
      "metadata": {
        "id": "62965710"
      },
      "source": [
        "Q5: Apply your implementation of gradient descent to the generated data to estimate the parameters. How close are they to the true parameters? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2526fe24",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter Comparison:\n",
            "True intercept (β₀): 1.0000, Estimated: 0.9562, Difference: 0.0438\n",
            "True β₁: 0.5000, Estimated: 0.5368, Difference: 0.0368\n",
            "True β₂: 2.0000, Estimated: 1.9948, Difference: 0.0052\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the same random seed as before for consistency\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate data with the specified parameters\n",
        "n = 1000\n",
        "p = 2\n",
        "true_beta = np.array([0.5, 2])\n",
        "true_intercept = 1\n",
        "\n",
        "# Generate features and outcomes\n",
        "X = generate_X(n, p)\n",
        "Y = generate_Y(X, true_beta, true_intercept)\n",
        "\n",
        "# Initialize parameters for gradient descent\n",
        "initial_betas = np.zeros(p + 1)  # +1 for intercept\n",
        "learning_rate = 0.1\n",
        "iterations = 5000\n",
        "\n",
        "# Run gradient descent\n",
        "estimated_betas, cost_history = run_gradient_descent(X, Y, learning_rate, iterations, initial_betas)\n",
        "\n",
        "# Extract the estimated intercept and coefficients\n",
        "estimated_intercept = estimated_betas[0]\n",
        "estimated_coefficients = estimated_betas[1:]\n",
        "\n",
        "# Compare true and estimated parameters\n",
        "print(\"Parameter Comparison:\")\n",
        "print(f\"True intercept (β₀): {true_intercept:.4f}, Estimated: {estimated_intercept:.4f}, Difference: {abs(true_intercept - estimated_intercept):.4f}\")\n",
        "print(f\"True β₁: {true_beta[0]:.4f}, Estimated: {estimated_coefficients[0]:.4f}, Difference: {abs(true_beta[0] - estimated_coefficients[0]):.4f}\")\n",
        "print(f\"True β₂: {true_beta[1]:.4f}, Estimated: {estimated_coefficients[1]:.4f}, Difference: {abs(true_beta[1] - estimated_coefficients[1]):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ede89a8",
      "metadata": {
        "id": "1ede89a8"
      },
      "source": [
        "\n",
        "Q6: Rerun your implementation of gradient descent but with a different initialization. Are the estimated parameters the same as that in Q5? (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "64120fdf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New random initialization: [-1.0856306   0.99734545  0.2829785 ]\n",
            "\n",
            "Parameter Comparison between Different Initializations:\n",
            "True parameters: Intercept=1.0000, β₁=0.5000, β₂=2.0000\n",
            "Q5 (Zero init): Intercept=0.9562, β₁=0.5368, β₂=1.9948\n",
            "Q6 (Random init): Intercept=0.9562, β₁=0.5368, β₂=1.9948\n",
            "\n",
            "Differences between Q5 and Q6 estimates:\n",
            "Intercept difference: 0.000000\n",
            "β₁ difference: 0.000000\n",
            "β₂ difference: 0.000000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the same random seed as before for consistency\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate data with the specified parameters (same as before)\n",
        "n = 1000\n",
        "p = 2\n",
        "true_beta = np.array([0.5, 2])\n",
        "true_intercept = 1\n",
        "\n",
        "# Generate features and outcomes (same as before)\n",
        "X = generate_X(n, p)\n",
        "Y = generate_Y(X, true_beta, true_intercept)\n",
        "\n",
        "# Store the previous results from Q5 for comparison\n",
        "# Assuming we have the estimated_betas from Q5\n",
        "estimated_betas_q5 = estimated_betas.copy()\n",
        "\n",
        "# New initialization: random values instead of zeros\n",
        "np.random.seed(123)  # Different seed for initialization\n",
        "initial_betas_random = np.random.normal(0, 1, p + 1)  # Random initialization\n",
        "print(f\"New random initialization: {initial_betas_random}\")\n",
        "\n",
        "# Run gradient descent with the new initialization\n",
        "learning_rate = 0.1\n",
        "iterations = 5000\n",
        "estimated_betas_new, cost_history_new = run_gradient_descent(X, Y, learning_rate, iterations, initial_betas_random)\n",
        "\n",
        "# Compare results from both initializations\n",
        "print(\"\\nParameter Comparison between Different Initializations:\")\n",
        "print(f\"True parameters: Intercept={true_intercept:.4f}, β₁={true_beta[0]:.4f}, β₂={true_beta[1]:.4f}\")\n",
        "print(f\"Q5 (Zero init): Intercept={estimated_betas_q5[0]:.4f}, β₁={estimated_betas_q5[1]:.4f}, β₂={estimated_betas_q5[2]:.4f}\")\n",
        "print(f\"Q6 (Random init): Intercept={estimated_betas_new[0]:.4f}, β₁={estimated_betas_new[1]:.4f}, β₂={estimated_betas_new[2]:.4f}\")\n",
        "\n",
        "# Calculate differences between the two estimates\n",
        "diff_intercept = abs(estimated_betas_q5[0] - estimated_betas_new[0])\n",
        "diff_beta1 = abs(estimated_betas_q5[1] - estimated_betas_new[1])\n",
        "diff_beta2 = abs(estimated_betas_q5[2] - estimated_betas_new[2])\n",
        "\n",
        "print(\"\\nDifferences between Q5 and Q6 estimates:\")\n",
        "print(f\"Intercept difference: {diff_intercept:.6f}\")\n",
        "print(f\"β₁ difference: {diff_beta1:.6f}\")\n",
        "print(f\"β₂ difference: {diff_beta2:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8e692c",
      "metadata": {
        "id": "2e8e692c"
      },
      "source": [
        "**Comparing your solution against scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0a4309",
      "metadata": {
        "id": "6c0a4309"
      },
      "source": [
        "Q7: Apply `sklearn.linear_model.LogisticRegressionÂ¶` to your generated data to estimate the parameters of a logistic regression model. ( 7 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0134caa6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter Comparison:\n",
            "True parameters: Intercept=1.0000, β₁=0.5000, β₂=2.0000\n",
            "sklearn estimates: Intercept=0.9562, β₁=0.5368, β₂=1.9948\n",
            "Our implementation: Intercept=0.9562, β₁=0.5368, β₂=1.9948\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize model\n",
        "model = LogisticRegression(fit_intercept=True, penalty = 'none', max_iter=1000)\n",
        "\n",
        "# Fit model using existing data from previous questions\n",
        "sklearn_model.fit(X, Y)  \n",
        "\n",
        "# Extract parameters\n",
        "sklearn_intercept = sklearn_model.intercept_[0]\n",
        "sklearn_coefficients = sklearn_model.coef_[0]\n",
        "\n",
        "# Compare with your implementation and true values\n",
        "print(\"Parameter Comparison:\")\n",
        "print(f\"True parameters: Intercept={true_intercept:.4f}, β₁={true_beta[0]:.4f}, β₂={true_beta[1]:.4f}\")\n",
        "print(f\"sklearn estimates: Intercept={sklearn_intercept:.4f}, β₁={sklearn_coefficients[0]:.4f}, β₂={sklearn_coefficients[1]:.4f}\")\n",
        "print(f\"Our implementation: Intercept={estimated_betas[0]:.4f}, β₁={estimated_betas[1]:.4f}, β₂={estimated_betas[2]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2491c54",
      "metadata": {
        "id": "d2491c54"
      },
      "source": [
        "Q8: Are the answers the same as that from your implementation? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b892a28a",
      "metadata": {},
      "source": [
        "Yes, the answers from our implementation are the same as the answers from sklearn."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
