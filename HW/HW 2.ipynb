{"cells":[{"cell_type":"markdown","id":"43e0c2ad-c4c1-42da-8181-6d76bec34e0e","metadata":{"id":"43e0c2ad-c4c1-42da-8181-6d76bec34e0e"},"source":["Welcome to Homework 2! The goal of this homework is for you to implement gradient boosted trees from scratch, to gain a deep appreciation for how the method works. To do this, you'll be able to use `DecisionTreeRegressor` in `sklearn`, but you won't be able to use the classes `GradientBoostingRegressor` or `GradientBoostingClassifier`.\n","\n","Then you'll apply this to the dataset \"Smoker Status Prediction using Bio-Signals\" on Kaggle https://www.kaggle.com/datasets/gauravduttakiit/smoker-status-prediction-using-biosignals"]},{"cell_type":"markdown","id":"fe7d2f60","metadata":{},"source":["Q1: Consider the task of training a gradient boosted tree to predict a binary outcome. Provide the mathematical definition of a gradient boosted tree $f$ with respect to a collection of regression trees $g_i$ for $i = 1,\\cdots, t$. Specify what quantity the gradient boosted tree predicts (i.e. probability, log odds, odds, etc). (10 points)"]},{"cell_type":"markdown","id":"e81c621b","metadata":{},"source":["Q2: As in logistic regression, we will use the cross-entropy loss (aka the negative log likelihood). Provide the objective function $L(f)$ for a gradient boosted tree $f$ that outputs a probability at any observation $x$ and a dataset of $n$ IID observations. (10 points)"]},{"cell_type":"markdown","id":"264cfd00-ec55-45ce-b999-3c86a475d348","metadata":{"id":"264cfd00-ec55-45ce-b999-3c86a475d348"},"source":["Q3: Given a gradient boosted tree at iteration $t$, i.e. $f_t$, what is the gradient of the objective function with respect $f_t$? That is, what is the gradient that we aim to aproximate in the next iteration of the fitting procedure? Derive it step by step. (10 points)"]},{"cell_type":"markdown","id":"39c1289e","metadata":{},"source":["Q4: Write a python function that fits a regression tree $g_{t+1}$ to estimate the gradient with respect to $f_t$. This function should take in the gradient, the features $x$, and max tree depth. (5 points)"]},{"cell_type":"markdown","id":"1d1de99e","metadata":{},"source":["Q5: Using the python function you defined in Q4, write a python function that fits a gradient boosting tree. The function should take in as arguments the max tree depth, the number of iterations, the training data, and the learning rate. The function should output your gradient boosted tree and the auc on the training data at each iteration. (20 points)"]},{"cell_type":"markdown","id":"b838b378","metadata":{},"source":["Q6: Load in the dataset. Split 75% of the data for training and the rest for testing. (5 points)"]},{"cell_type":"markdown","id":"75eb4afc","metadata":{},"source":["Q7: Apply your gradient boosted tree to this dataset. Plot the training auc over each iteration. (5 points)"]},{"cell_type":"markdown","id":"5ead9c3a","metadata":{},"source":["Q8: What is the AUC of your gradient boosted tree on the test data? (5 points)"]},{"cell_type":"markdown","id":"2758f58f","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}