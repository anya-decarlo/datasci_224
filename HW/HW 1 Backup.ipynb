{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e",
      "metadata": {
        "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e"
      },
      "source": [
        "Welcome to Homework 1! Your task for this homework is to implement logistic regression from scratch. You are welcome to use existing software packages to check your work. However, implementing it once in your life will help you better understand how this algorithm works.\n",
        "\n",
        "Recall: Logistic regression is a classification model that estimates the probability of a binary outcome $Y$ being equal to 1, given variables/features $X$. It assumes that the log odds is linear with respect to $X$. Because it can be viewed as a generalization of linear regression, it falls under the general umbrella of methods called \"generalizaed linear models.\"\n",
        "\n",
        "Helpful resources:\n",
        "* https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "\n",
        "Concepts you'll need from lectures:\n",
        "* Maximum likelihood estimation\n",
        "* Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6451ab",
      "metadata": {},
      "source": [
        "### Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264cfd00-ec55-45ce-b999-3c86a475d348",
      "metadata": {
        "id": "264cfd00-ec55-45ce-b999-3c86a475d348"
      },
      "source": [
        "**How does Logistic Regression work?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecad7236",
      "metadata": {
        "id": "ecad7236"
      },
      "source": [
        "Q1: What is the mathematical equation that describes the probability distribution of a binary random variable? (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c89a90",
      "metadata": {},
      "source": [
        "$p(x; q) = q^x(1-q)^{1-x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5562197",
      "metadata": {
        "id": "f5562197"
      },
      "source": [
        "Q2: What probability distribution does logistic regression assume $Y|X$ follows? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc7af92",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "Given data $(X_i, Y_i)$, the log likelihood of any given paramater is:\n",
        "\n",
        "$$\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^N \\log \\Pr(Y_i \\mid X_i; \\theta)$$\n",
        "\n",
        "\n",
        "First, we use the log liklehood for N observations:\n",
        "\n",
        "$$\\ell(\\beta) = \\sum_{i=1}^{N} \\log p_{g_i}(x_i; \\beta)$$\n",
        "\n",
        " Which written to binary logistic regression form is:\n",
        "\n",
        "$$\\ell(\\beta) = \\sum_{i=1}^{N} \\left[ y_i \\log p(x_i;\\beta) + (1-y_i) \\log(1-p(x_i;\\beta)) \\right]$$\n",
        "\n",
        "* For binary logistic regression, where $g_i = y_i \\in \\{0,1\\}$.\n",
        "\n",
        "* Where $p(x_i;\\beta) = \\frac{1}{1 + e^{-\\beta^T x_i}}$ is the probability that $Y_i = 1$ given $X_i$.\n",
        "\n",
        "Given data $(X_i, Y_i)$ for $i=1,\\cdots, n$ the log likelihood for binary logistic regression parameters $\\beta$ is defined as:\n",
        "$$\\ell(\\beta) = \\sum_{i=1}^{    N} \\left[ y_i \\beta^T x_i - \\log(1+e^{\\beta^T x_i}) \\right]$$\n",
        "\n",
        "* $\\mathbf{W}$ is a $N \\times N$ diagonal matrix of weights where:\n",
        "  * The ith diagonal element is $p(x_i;\\beta^{\\text{old}})(1 - p(x_i; \\beta^{\\text{old}}))$\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a64e62b",
      "metadata": {
        "id": "4a64e62b"
      },
      "source": [
        "Q3: What are the parameters of a logistic regression model? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0761a7c9",
      "metadata": {
        "id": "0761a7c9"
      },
      "source": [
        "Q4: What is the log likelihood of the parameters given observations $(X_i, Y_i)$ for $i=1,\\cdots, n$? (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e2aba8",
      "metadata": {
        "id": "b5e2aba8"
      },
      "source": [
        "Q5: What is the optimization problem that we try to solve when fitting logistic regression?  (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6990b0d",
      "metadata": {
        "id": "c6990b0d"
      },
      "source": [
        "Q6: What procedures can be used to solve the optimization problem underlying logistic regression? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d73b5d",
      "metadata": {
        "id": "81d73b5d"
      },
      "source": [
        "Q7: Derive the gradient of the log likelihood with respect to the parameters of the logistic regression model step by step. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ff04c8",
      "metadata": {},
      "source": [
        "### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f",
      "metadata": {
        "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f"
      },
      "source": [
        "**Implement Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008739de",
      "metadata": {
        "id": "008739de"
      },
      "source": [
        "Q1: Write the function `generate_X(n,p)`, which returns randomly generated $X_1,\\cdots, X_n$, where $X_i \\in \\mathbb{R}^p$. You can sample the variables using a uniform distribution or a standard normal distribution. (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4598cf",
      "metadata": {
        "id": "cb4598cf"
      },
      "source": [
        "Q2: Write the function `generate_Y(X, beta, intercept)`, which generates outcomes for observations $X_1,\\cdots, X_p$ per a logistic regression model with coefficients $\\beta \\in \\mathbb{R}^{p}$ and intercept $\\beta_0$. (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e66ccc4",
      "metadata": {
        "id": "8e66ccc4"
      },
      "source": [
        "Q3: Generate some data using your functions above with $p=2$, $n=1000$, coefficients $\\beta=(0.5,2)$, and intercept $\\beta_0 = 1$. (7 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5a40b8",
      "metadata": {
        "id": "0c5a40b8"
      },
      "source": [
        "Q4: Implement a function that runs gradient descent `run_gradient_descent(X, Y, alpha, num_iterations, initial_betas)`. Make sure to vectorize your code. (Otherwise it will run really slowly.) (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62965710",
      "metadata": {
        "id": "62965710"
      },
      "source": [
        "Q5: Apply your implementation of gradient descent to the generated data to estimate the parameters. How close are they to the true parameters? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba360878",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Plot the cost history to check convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost (Negative Log Likelihood)')\n",
        "plt.title('Cost History During Gradient Descent')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the data with both true and estimated decision boundaries\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0', alpha=0.6)\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], label='Class 1', alpha=0.6)\n",
        "\n",
        "# Plot the true decision boundary\n",
        "x1_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
        "x2_true_boundary = -(true_intercept + true_beta[0] * x1_range) / true_beta[1]\n",
        "plt.plot(x1_range, x2_true_boundary, 'r--', label='True Decision Boundary')\n",
        "\n",
        "# Plot the estimated decision boundary\n",
        "x2_estimated_boundary = -(estimated_intercept + estimated_coefficients[0] * x1_range) / estimated_coefficients[1]\n",
        "plt.plot(x1_range, x2_estimated_boundary, 'g-', label='Estimated Decision Boundary')\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Data with True and Estimated Decision Boundaries')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ede89a8",
      "metadata": {
        "id": "1ede89a8"
      },
      "source": [
        "\n",
        "Q6: Rerun your implementation of gradient descent but with a different initialization. Are the estimated parameters the same as that in Q5? (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0e2877",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Plot cost histories for both initializations\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cost_history, label='Q5 (Zero Initialization)')\n",
        "plt.plot(cost_history_new, label='Q6 (Random Initialization)')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost (Negative Log Likelihood)')\n",
        "plt.title('Cost History Comparison for Different Initializations')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the decision boundaries from both initializations\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0', alpha=0.6)\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], label='Class 1', alpha=0.6)\n",
        "\n",
        "# Plot the true decision boundary\n",
        "x1_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
        "x2_true_boundary = -(true_intercept + true_beta[0] * x1_range) / true_beta[1]\n",
        "plt.plot(x1_range, x2_true_boundary, 'k--', label='True Decision Boundary')\n",
        "\n",
        "# Plot the Q5 decision boundary\n",
        "x2_q5_boundary = -(estimated_betas_q5[0] + estimated_betas_q5[1] * x1_range) / estimated_betas_q5[2]\n",
        "plt.plot(x1_range, x2_q5_boundary, 'r-', label='Q5 Decision Boundary')\n",
        "\n",
        "# Plot the Q6 decision boundary\n",
        "x2_q6_boundary = -(estimated_betas_new[0] + estimated_betas_new[1] * x1_range) / estimated_betas_new[2]\n",
        "plt.plot(x1_range, x2_q6_boundary, 'g-', label='Q6 Decision Boundary')\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Decision Boundaries with Different Initializations')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8e692c",
      "metadata": {
        "id": "2e8e692c"
      },
      "source": [
        "**Comparing your solution against scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0a4309",
      "metadata": {
        "id": "6c0a4309"
      },
      "source": [
        "Q7: Apply `sklearn.linear_model.LogisticRegressionÂ¶` to your generated data to estimate the parameters of a logistic regression model. ( 7 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7606c366",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use the same random seed as before for consistency\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate data with the specified parameters (same as before)\n",
        "n = 1000\n",
        "p = 2\n",
        "true_beta = np.array([0.5, 2])\n",
        "true_intercept = 1\n",
        "\n",
        "# Generate features and outcomes (same as before)\n",
        "X = generate_X(n, p)\n",
        "Y = generate_Y(X, true_beta, true_intercept)\n",
        "\n",
        "# Apply scikit-learn's LogisticRegression\n",
        "sklearn_model = LogisticRegression(fit_intercept=True, C=1e10, solver='liblinear', max_iter=1000)\n",
        "sklearn_model.fit(X, Y)\n",
        "\n",
        "# Extract the estimated parameters\n",
        "sklearn_intercept = sklearn_model.intercept_[0]\n",
        "sklearn_coefficients = sklearn_model.coef_[0]\n",
        "\n",
        "# Compare true and sklearn-estimated parameters\n",
        "print(\"Parameter Comparison:\")\n",
        "print(f\"True parameters: Intercept={true_intercept:.4f}, β₁={true_beta[0]:.4f}, β₂={true_beta[1]:.4f}\")\n",
        "print(f\"sklearn estimates: Intercept={sklearn_intercept:.4f}, β₁={sklearn_coefficients[0]:.4f}, β₂={sklearn_coefficients[1]:.4f}\")\n",
        "\n",
        "# Calculate differences between true and sklearn-estimated parameters\n",
        "diff_intercept = abs(true_intercept - sklearn_intercept)\n",
        "diff_beta1 = abs(true_beta[0] - sklearn_coefficients[0])\n",
        "diff_beta2 = abs(true_beta[1] - sklearn_coefficients[1])\n",
        "\n",
        "print(\"\\nDifferences between true and sklearn-estimated parameters:\")\n",
        "print(f\"Intercept difference: {diff_intercept:.6f}\")\n",
        "print(f\"β₁ difference: {diff_beta1:.6f}\")\n",
        "print(f\"β₂ difference: {diff_beta2:.6f}\")\n",
        "\n",
        "# Visualize the data with true and sklearn decision boundaries\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0', alpha=0.6)\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], label='Class 1', alpha=0.6)\n",
        "\n",
        "# Plot the true decision boundary\n",
        "x1_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
        "x2_true_boundary = -(true_intercept + true_beta[0] * x1_range) / true_beta[1]\n",
        "plt.plot(x1_range, x2_true_boundary, 'r--', label='True Decision Boundary')\n",
        "\n",
        "# Plot the sklearn decision boundary\n",
        "x2_sklearn_boundary = -(sklearn_intercept + sklearn_coefficients[0] * x1_range) / sklearn_coefficients[1]\n",
        "plt.plot(x1_range, x2_sklearn_boundary, 'g-', label='sklearn Decision Boundary')\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Data with True and sklearn Decision Boundaries')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Compare predictions between true model and sklearn model\n",
        "true_probs = 1 / (1 + np.exp(-(true_intercept + np.dot(X, true_beta))))\n",
        "sklearn_probs = sklearn_model.predict_proba(X)[:, 1]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(true_probs, sklearn_probs, alpha=0.5)\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlabel('True Model Probabilities')\n",
        "plt.ylabel('sklearn Model Probabilities')\n",
        "plt.title('Comparison of Predicted Probabilities')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2491c54",
      "metadata": {
        "id": "d2491c54"
      },
      "source": [
        "Q8: Are the answers the same as that from your implementation? (1 point)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
